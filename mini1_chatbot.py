import os
import torch
import pandas as pd
import pinecone  # ê³µì‹ Pinecone í´ë¼ì´ì–¸íŠ¸
from pinecone import ServerlessSpec
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_pinecone import Pinecone as LC_Pinecone
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from tiktoken import encoding_for_model, get_encoding
from functools import lru_cache
from dotenv import load_dotenv
import re
import streamlit as st
import random

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ----------------------------------------------------------------------------------
# 1) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_region = os.getenv("PINECONE_ENVIRONMENT")  # ì˜ˆ: "us-east-1"
index_name = os.getenv("INDEX_NAME")
gemini_api_key = os.getenv("GEMINI_API_KEY")

if not pinecone_api_key or not pinecone_region:
    raise ValueError("Pinecone API í‚¤ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not index_name:
    raise ValueError("INDEX_NAME í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not gemini_api_key:
    raise ValueError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ----------------------------------------------------------------------------------
# 2) Pinecone í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± & ì¸ë±ìŠ¤ ì¤€ë¹„
pc = pinecone.Pinecone(api_key=pinecone_api_key)
index = pc.Index(index_name)

# ----------------------------------------------------------------------------------
# 3) ì¥ì¹˜ ì„¤ì • (GPU/CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# 4) LangChain ì„ë² ë”© & VectorStore
embed_model_name = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"
embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)

vectorstore = LC_Pinecone.from_existing_index(
    index_name=index_name,
    embedding=embeddings,
    text_key="text"  
)

# 5) Google Generative AI (Gemini-1.5-flash) LLM ìƒì„±
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.3,
    top_p=0.7,
    top_k=50,
    api_key=gemini_api_key
)

# ë§¨ ìœ„ìª½(ì „ì—­)ì— ì¶”ê°€
if "already_greeted" not in st.session_state:
    st.session_state.already_greeted = False

def remove_repeated_greeting(text: str) -> str:
    """
    ì´ë¯¸ ì¸ì‚¬í–ˆìœ¼ë©´, ë‹µë³€ì—ì„œ 'ì•ˆë…•í•˜ì„¸ìš”' ë¬¸êµ¬ë¥¼ ì œê±°í•œë‹¤.
    ì•„ì§ ì¸ì‚¬ ì „ì´ë©´, ì¸ì‚¬ ë°œê²¬ ì‹œì ë¶€í„° 'ì´ë¯¸ ì¸ì‚¬í•œ ìƒíƒœ'ë¡œ ë°”ê¾¼ë‹¤.
    """
    if not st.session_state.already_greeted:
        # ë§Œì•½ ì´ë²ˆ ë‹µë³€ì— 'ì•ˆë…•í•˜ì„¸ìš”'ê°€ ìˆìœ¼ë©´, ì¸ì‚¬ ìƒíƒœ Trueë¡œ ì „í™˜
        if "ì•ˆë…•í•˜ì„¸ìš”" in text:
            st.session_state.already_greeted = True
        return text
    else:
        # ì´ë¯¸ ì¸ì‚¬ë¥¼ í–ˆë‹¤ë©´, ë°˜ë³µ 'ì•ˆë…•í•˜ì„¸ìš”'ë¥¼ ì œê±°
        # í•„ìš”í•˜ë‹¤ë©´ ì •ê·œì‹ìœ¼ë¡œ ë” ê¼¼ê¼¼íˆ ì²˜ë¦¬ ê°€ëŠ¥
        new_text = text.replace("ì•ˆë…•í•˜ì„¸ìš”! ", "").replace("ì•ˆë…•í•˜ì„¸ìš”!", "")
        return new_text




# 6) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ 
system_prompt = """
ë„ˆëŠ” ì¼ë³¸ ì• ë‹ˆë©”ì´ì…˜ ë° ê³µí¬ì˜í™” ì¶”ì²œ ë° ì •ë³´ ì œê³µ ë° ì¶”ì²œ ì „ë¬¸ê°€ì•¼.

1. ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ë©´ í•´ë‹¹ ë¶„ì•¼ì˜ ì‘í’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œê³¼ ìƒì„¸ ì •ë³´ë¥¼ ì œê³µí•´.
2. ì¶”ì²œ ì •ë³´ëŠ” ì‘í’ˆ ì œëª©, ì¥ë¥´, ì¤„ê±°ë¦¬, í‰ì , ê°œë´‰ ë‚ ì§œ ë“± ì£¼ìš” ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•´.
3. ì´ì „ì— ì¶”ì²œí•œ ì‘í’ˆì€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ìƒˆë¡œìš´ ì‘í’ˆì„ ì¶”ì²œí•´ì¤˜.
4. ë§Œì•½ì— ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, ì¶”ê°€ ì§ˆë¬¸ì„ í•˜ë„ë¡ í•´.
5. ì‚¬ìš©ìì˜ ì´ì „ ì§ˆë¬¸ì„ ê¸°ì–µí•´ì„œ ë§ì¶¤í™”ëœ ë‹µë³€ì„ ì œê³µí•´.
6. í•œ ë²ˆ ì¸ì‚¬ë¥¼ í•œ ì´í›„ì—ëŠ” ê°™ì€ ëŒ€í™”ì—ì„œ 'ì•ˆë…•í•˜ì„¸ìš”!'ë¼ê³  ì¸ì‚¬í•˜ì§€ë§ˆ.
6. ë‹µë³€ì€ ì¹œê·¼í•˜ê³  ë”°ë“¯í•œ ì–´ì¡°ë¡œ ì‘ì„±í•´ì¤˜.

"""

try:
    global_encoding = encoding_for_model("gemini-1.5-flash")
except KeyError:
    global_encoding = get_encoding("cl100k_base")

@lru_cache(maxsize=10000)
def cached_token_count(text):
    return len(global_encoding.encode(text))

def count_tokens_text(text: str) -> int:
    return len(global_encoding.encode(text))

# ëŒ€í™” ì´ë ¥ ê´€ë ¨ ë³€ìˆ˜ (ë©”ëª¨ë¦¬ ë‚´ ì €ì¥)
response_cache = {}
conversation_history = {}
user_llm_calls = {}

def initialize_conversation(user_id: str, system_prompt: str):
    prev = conversation_history.get(user_id, {"messages": [], "filters": {}, "recommended_ids": set()})
    if not prev["messages"]:
        prev["messages"].append(SystemMessage(content=system_prompt))
    conversation_history[user_id] = prev
    return prev

def update_conversation_history(user_id, new_message, filters):
    prev = conversation_history.get(user_id, {"messages": [], "filters": {}, "recommended_ids": set()})
    prev["messages"].append(new_message)
    prev["filters"] = filters
    conversation_history[user_id] = prev

def summarize_conversation(user_id: str, max_messages: int = 10):
    hist = conversation_history.get(user_id)
    if not hist: 
        return
    msgs = hist["messages"]
    if len(msgs) <= max_messages:
        return
    system_msg = msgs[0]
    last_msg = msgs[-1]
    middle = msgs[1:-1]
    if not middle:
        return

    user_llm_calls[user_id] = user_llm_calls.get(user_id, 0)
    if user_llm_calls[user_id] >= 3:
        return

    conversation_text = "\n".join([f"{m.__class__.__name__}: {m.content}" for m in middle])
    prompt = f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜:\n{conversation_text}"
    try:
        summary_result = llm.invoke([HumanMessage(content=prompt)])
        user_llm_calls[user_id] += 1
    except Exception as e:
        print("ìš”ì•½ ì˜¤ë¥˜:", e)
        return

    summary_msg = SystemMessage(content=f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_result.content}")
    new_msgs = [system_msg, summary_msg, last_msg]
    hist["messages"] = new_msgs
    conversation_history[user_id] = hist

def create_search_filter(query: str):
    filters = {}
    query_lower = query.lower()

    category_map = {
        "ê³µí¬": "ê³µí¬ì˜í™”",
        "í˜¸ëŸ¬": "ê³µí¬ì˜í™”",
        "ì• ë‹ˆ": "ì• ë‹ˆë©”ì´ì…˜",
        "ë§Œí™”": "ì• ë‹ˆë©”ì´ì…˜",
        "ì¼ë³¸": "ì• ë‹ˆë©”ì´ì…˜",
        "ì• ë‹ˆë©”ì´ì…˜": "ì• ë‹ˆë©”ì´ì…˜"
    }

    matched_categories = set()
    for kw, cat in category_map.items():
        if kw in query_lower:
            matched_categories.add(cat)

    if len(matched_categories) == 1:
        filters["ì¹´í…Œê³ ë¦¬"] = {"$eq": matched_categories.pop()}
    elif len(matched_categories) > 1:
        filters["ì¹´í…Œê³ ë¦¬"] = {"$in": list(matched_categories)}

    year_pattern = re.findall(r"(\d{4})ë…„", query_lower)
    if year_pattern:
        year_int = int(year_pattern[0])
        filters["ê°œë´‰ ë‚ ì§œ"] = {"$eq": year_int}

    if "ìµœê·¼" in query_lower:
        now_year = 2023
        lower_year_bound = now_year - 2
        filters["ê°œë´‰ ë‚ ì§œ"] = {"$gte": lower_year_bound}

    decade_pattern = re.search(r"(\d{4})ë…„ëŒ€", query_lower)
    if decade_pattern:
        decade_start = int(decade_pattern.group(1))
        filters["ê°œë´‰ ë‚ ì§œ"] = {"$gte": decade_start, "$lte": decade_start + 9}

    rating_match = re.findall(r"(ì „ì²´|13ì„¸|15ì„¸|18ì„¸|19ê¸ˆ)", query_lower)
    if rating_match:
        rating = rating_match[0]
        filters["ì‹œì²­ ë“±ê¸‰"] = {"$eq": rating}

    rating_pattern = re.search(r"(í‰ì |ì ìˆ˜)\s*(\d+(\.\d+)?)(ì |ì ëŒ€| )?\s*(ì´ìƒ|ì´í•˜|ì´ˆê³¼|ë¯¸ë§Œ)?", query_lower)
    if rating_pattern:
        rating_str = rating_pattern.group(2)
        compare_keyword = rating_pattern.group(5)
        rating_val = float(rating_str)
        if compare_keyword == "ì´ìƒ" or compare_keyword == "ì´ˆê³¼":
            filters["ì ìˆ˜"] = {"$gte": rating_val}
        elif compare_keyword == "ì´í•˜" or compare_keyword == "ë¯¸ë§Œ":
            filters["ì ìˆ˜"] = {"$lte": rating_val}
        else:
            filters["ì ìˆ˜"] = {"$eq": rating_val}

    return filters

def rank_documents_by_score(docs, weight_score=0.7, weight_votes=0.3):
    def compute_rank(doc):
        score = float(doc.metadata.get("ì ìˆ˜", 0))
        votes = float(doc.metadata.get("íˆ¬í‘œìˆ˜", 0))
        return weight_score * score + weight_votes * votes

    sorted_docs = sorted(docs, key=lambda doc: compute_rank(doc), reverse=True)
    return sorted_docs

def retrieve_documents(query: str, filters: dict, k=5, exclude_ids=None):
    if exclude_ids is None:
        exclude_ids = set()
    try:
        docs = vectorstore.similarity_search(query, k=k*2, filter=filters)
    except Exception as e:
        print("Pinecone ê²€ìƒ‰ ì˜¤ë¥˜:", e)
        return []

    filtered_docs = []
    for doc in docs:
        doc_id = doc.metadata.get("id")
        if doc_id not in exclude_ids:
            filtered_docs.append(doc)
        if len(filtered_docs) >= k:
            break

    query_lower = query.lower()
    if "í‰ì " in query_lower or "ì ìˆ˜" in query_lower:
        filtered_docs = rank_documents_by_score(filtered_docs, weight_score=0.7, weight_votes=0.3)
        filtered_docs = filtered_docs[:k]

    return filtered_docs

def create_prompt(system_prompt, prev_text, question, context):
    return f"""
{system_prompt}

ì´ì „ ëŒ€í™” ë‚´ìš©:
{prev_text}

ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ë‚´ìš©:
{question}

ê²€ìƒ‰ëœ ì‘í’ˆ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì¤˜.
"""

def reduce_context(docs, prev_text, system_prompt, question, max_token_limit=5000):
    while len(docs) > 0:
        docs = docs[:-1]
        context = "\n\n".join([
            "\n".join([f"{k}: {v}" for k,v in doc.metadata.items()])
            for doc in docs
        ])
        prompt = create_prompt(system_prompt, prev_text, question, context)
        total_tokens = count_tokens_text(prompt)
        if total_tokens <= max_token_limit:
            return prompt, docs
    raise ValueError("í† í° ì œí•œ ì´ˆê³¼")






def call_llm(llm, prompt_with_context):
    try:
        resp = llm.invoke([HumanMessage(content=prompt_with_context)])
        return resp
    except Exception as e:
        print("LLM í˜¸ì¶œ ì˜¤ë¥˜:", e)
        return AIMessage(content="ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


def query_llm(user_id: str, question: str, forced_category: str):

    # 1) ëŒ€í™” ì´ˆê¸°í™”
    initialize_conversation(user_id, system_prompt)
    
    # 2) ê²€ìƒ‰ í•„í„° ìƒì„±
    filters = create_search_filter(question)


    # (ì¤‘ìš”) ì‚¬ìš©ìê°€ ë¼ë””ì˜¤ ë²„íŠ¼ìœ¼ë¡œ ê³ ë¥¸ ì¥ë¥´ë¥¼ 'ì¹´í…Œê³ ë¦¬' í•„ë“œì— ê°•ì œ ì£¼ì…
    filters["ì¹´í…Œê³ ë¦¬"] = {"$eq": forced_category}

    # 3) ì´ì „ ëŒ€í™” ìš”ì•½
    summarize_conversation(user_id, max_messages=10)
    hist = conversation_history[user_id]

    # ì‚¬ìš©ìê°€ ë³´ë‚¸ ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    hist["messages"].append(HumanMessage(content=question))
    recommended_ids = hist.get("recommended_ids", set())

    # 4) Pinecone ë“± VectorStoreì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
    docs = retrieve_documents(question, filters, k=5, exclude_ids=recommended_ids)

    # 5) ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´
    if not docs:
        no_result = f"'{forced_category}' ì¹´í…Œê³ ë¦¬ì—ì„œ ì‘í’ˆì„ ì°¾ì„ ìˆ˜ ì—†ë„¤ìš”!"
        update_conversation_history(user_id, AIMessage(content=no_result), filters)
        response_cache[user_id] = no_result
        return no_result

    # 6) ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
    context = "\n\n".join([
        "\n".join([
            f"ì›ì œëª©: {doc.metadata.get('ì›ì œëª©','ì—†ìŒ')}",
            f"í•œê¸€ì œëª©: {doc.metadata.get('í•œê¸€ì œëª©','ì—†ìŒ')}",
            f"íƒ€ì…: {doc.metadata.get('íƒ€ì…','ì—†ìŒ')}",
            f"ì¥ë¥´: {doc.metadata.get('ì¥ë¥´','ì—†ìŒ')}",
            f"ì¤„ê±°ë¦¬: {doc.metadata.get('ì¤„ê±°ë¦¬','ì—†ìŒ')[:100]}...",
            f"ì ìˆ˜: {doc.metadata.get('ì ìˆ˜','ì—†ìŒ')}",
            f"ê°œë´‰ ë‚ ì§œ: {doc.metadata.get('ê°œë´‰ ë‚ ì§œ','ì—†ìŒ')}",
            f"id: {doc.metadata.get('id','')}",
        ])
        for doc in docs
    ])

    prev_msgs_text = "\n\n".join([f"{m.__class__.__name__}: {m.content}" for m in hist["messages"]])
    prompt_with_context = create_prompt(system_prompt, prev_msgs_text, question, context)

    # 7) í† í° ì œí•œ ì²´í¬ í›„ í•„ìš” ì‹œ ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ
    total_tokens = count_tokens_text(prompt_with_context)
    if total_tokens > 5000:
        prompt_with_context, docs = reduce_context(docs, prev_msgs_text, system_prompt, question, 5000)

    # 8) LLM í˜¸ì¶œ
    resp = call_llm(llm, prompt_with_context)
    final_answer = resp.content if isinstance(resp, AIMessage) else resp.content

    # --- ì—¬ê¸°ì„œ "ì•ˆë…•í•˜ì„¸ìš”" ì¤‘ë³µ ë°©ì§€ í›„ì²˜ë¦¬ ---
    final_answer = remove_repeated_greeting(final_answer)



    # 9) ì¶”ì²œëœ ë¬¸ì„œ IDë¥¼ ê¸°ë¡ (ì¤‘ë³µ ì¶”ì²œ ë°©ì§€ìš©)
    for d in docs:
        rid = d.metadata.get("id")
        if rid:
            recommended_ids.add(rid)
    hist["recommended_ids"] = recommended_ids

    # 10) ëŒ€í™” ì´ë ¥ê³¼ ìºì‹œì— ìµœì¢… ë‹µë³€ ì—…ë°ì´íŠ¸
    update_conversation_history(user_id, AIMessage(content=final_answer), filters)
    response_cache[user_id] = final_answer
    
    return final_answer

# ----------------------------------------------------------------------------------
# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="ì¼ë³¸ ì• ë‹ˆë©”ì´ì…˜ & ê³µí¬ ì˜í™” ì¶”ì²œ ì±—ë´‡", layout="wide")
st.title("ğŸ­ Anime & Horror ê°™ì´ ë³´ì•„~ ì¦ˆ! ğŸ‘»ğŸŒŸ")

# ìƒíƒœ ì €ì¥ (ëŒ€í™” ê¸°ë¡, ë§ˆì§€ë§‰ ì¥ë¥´, í…Œë§ˆ, ì‚¬ìš©ì ID)
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "last_genre" not in st.session_state:
    st.session_state.last_genre = "ê³µí¬ì˜í™”"  # ê¸°ë³¸ê°’
if "theme" not in st.session_state:
    st.session_state.theme = "light"
if "user_id" not in st.session_state:
    st.session_state.user_id = "streamlit_user"

# ì‚¬ìš©ìì—ê²Œ ì¥ë¥´ ì„ íƒ ìš”ì²­
choice = st.radio("ì¥ë¥´ ì„ íƒ", ["ê³µí¬ì˜í™”", "ì• ë‹ˆë©”ì´ì…˜"], key="genre_choice")

# ì‚¬ìš©ìê°€ ì¥ë¥´ë¥¼ ê³ ë¥´ë©´ ì„¸ì…˜ì— ë°˜ì˜
st.session_state.last_genre = choice

# í…Œë§ˆ ë³€ê²½ í•¨ìˆ˜ (ë°°ê²½ & ê¸€ì”¨ìƒ‰)
def set_theme(theme, sidebar_bg_color):
    bg_color = "#df3dff" if theme == "dark" else "#fffdba"
    text_color = "#FFFFFF" if theme == "dark" else "#000000"
    button_bg_color = "#FFFFFF"
    button_text_color = "#000000"
    
    st.markdown(
        f"""
        <style>
            .stApp {{
                background-color: {bg_color} !important;
                color: {text_color} !important;
            }}
            .stRadio {{
                background-color: transparent !important;
            }}
            .stRadio label {{
                color: {button_text_color} !important;
                font-weight: bold;
                padding: 10px;
                display: inline-block;
            }}
            .stRadio div {{
                color: {text_color} !important;
            }}
            .stButton button {{
                background-color: {button_bg_color} !important;
                color: {button_text_color} !important;
                font-weight: bold;
                border-radius: 5px;
            }}
            .sidebar {{
                background-color: {sidebar_bg_color} !important;
                padding: 15px;
                border-radius: 10px;
            }}
            .stChatMessage .stMarkdown p {{
                color: {text_color} !important;
            }}
        </style>
        """,
        unsafe_allow_html=True
    )
    return text_color

if choice == "ê³µí¬ì˜í™”":
    text_color = set_theme("dark", "#D1B3FF")
    st.session_state.last_genre = "ê³µí¬ì˜í™”"
elif choice == "ì• ë‹ˆë©”ì´ì…˜":
    text_color = set_theme("light", "#FFEB99")
    st.session_state.last_genre = "ì• ë‹ˆë©”ì´ì…˜"




# ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ì±—ë´‡ ì†Œê°œ ì¶”ê°€
with st.sidebar:
    if st.session_state.get("last_genre") == "ì• ë‹ˆë©”ì´ì…˜":
        st.markdown("""<div class='sidebar'>
            <h3>ğŸ¬ Anime Movie Bot</h3>
            <p>ì• ë‹ˆë©”ì´ì…˜ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤! ë‚˜ë§Œì˜ ì·¨í–¥ì €ê²© ì• ë‹ˆë©”ì´ì…˜ì„ ì°¾ì•„ë³´ì„¸ìš”! ë‹¤ì–‘í•œ ì¥ë¥´ë¥¼ ì…ë ¥í•´ë³´ì„¸ìš”!</p>
            <img src='https://cdn.pixabay.com/animation/2023/11/01/09/27/09-27-12-411_512.gif' width='150'>
        </div>""", unsafe_allow_html=True)
    elif st.session_state.get("last_genre") == "ê³µí¬ì˜í™”":
        st.markdown("""<div class='sidebar'>
            <h3>ğŸ¬ Horror Movie Bot</h3>
            <p>ê³µí¬ì˜í™” ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤! ê³µí¬ì˜í™”ê°€ í•„ìš”í•œ ìˆœê°„ì— ì í•©í•œ ì˜í™”ë¥¼ ì¶”ì²œí•´ë“œë ¤ìš”!</p>
            <img src='https://cdn.pixabay.com/animation/2022/10/13/14/20/14-20-47-878_512.gif' width='150'>
        </div>""", unsafe_allow_html=True)
    else:
        st.markdown("""<div class='sidebar'>
            <h3>ğŸ¬ Choose a Genre!</h3>
            <p>ì˜í™” ì¥ë¥´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.</p>
        </div>""", unsafe_allow_html=True)

# ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ğŸ”„ ì±„íŒ… ì´ˆê¸°í™”"):
    st.session_state.chat_history = []
    st.session_state.last_genre = "ê³µí¬ì˜í™”"
    st.session_state.theme = "light"
    st.session_state.user_id = "streamlit_user"
    st.session_state.clear()
    st.rerun()

# ì´ì „ ëŒ€í™” í‘œì‹œ
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for content in st.session_state.chat_history:
    with st.chat_message(content["role"]):
        if content["role"] == "ai" and st.session_state.last_genre == "ê³µí¬ì˜í™”":
            st.markdown(f'<p style="color: #FFFFFF;">{content["message"]}</p>', unsafe_allow_html=True)
        else:
            st.markdown(content["message"])


# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.chat_history.append({"role": "user", "message": prompt})

    # LLM í˜¸ì¶œ (ì˜ˆì‹œ)
    response = query_llm(st.session_state.get("user_id", "test_user"), prompt, st.session_state.last_genre)

    # ì±—ë´‡ ë‹µë³€ (ê³µí¬ì˜í™” -> í°ê¸€ì”¨, ì• ë‹ˆ -> ê¸°ë³¸ê°’)
    with st.chat_message("ai"):
        if st.session_state.last_genre == "ê³µí¬ì˜í™”":  # ê³µí¬ì˜í™”ì¼ ê²½ìš°
            st.markdown(f'<p style="color: #FFFFFF;">{response}</p>', unsafe_allow_html=True)
        else:  # ì• ë‹ˆë©”ì´ì…˜ì¼ ê²½ìš°
            st.markdown(f'<p style="color: #000000;">{response}</p>', unsafe_allow_html=True)

    st.session_state.chat_history.append({"role": "ai", "message": response})

    


# ì˜¤ë¥¸ìª½ ìƒë‹¨ì— ì´ë¯¸ì§€ ì¶”ê°€
col1, col2 = st.columns([3, 1])
with col2:
    if choice == "ê³µí¬ì˜í™”":
        st.markdown("""<div style="position: fixed; top: 100px; right: 10px;">
            <img src="https://cdn.pixabay.com/animation/2025/01/27/23/38/23-38-11-320_512.gif" width="250" alt="ê³µí¬ì˜í™”ì˜ ì„¸ê³„">
        </div>""", unsafe_allow_html=True)
    elif choice == "ì• ë‹ˆë©”ì´ì…˜":
        anime_images = [
            "https://mblogthumb-phinf.pstatic.net/MjAyMTA0MjZfMjcx/MDAxNjE5MzY1Mjg1MzUz.bqKdXoiQfDvluBmKLvnRnHpC7c9evgcimqI2nvB4CoYg.czF81H9WfvdUHJDOrEdkBg_ZG6xo6yMjToRGnssqr58g.GIF.badtutle124/%EC%A7%80%EB%A6%BC2.gif?type=w800",
            "https://img.onnada.com/201703/1794831217_11697e17_22.gif",
            "https://blog.kakaocdn.net/dn/A4Jys/btrsSvkfSir/JdfPZxZ5LA9kGz3gsg3LcK/img.gif",
            "https://i.pinimg.com/originals/2f/e4/32/2fe432a6fb2eda99bee011c12027a500.gif",
            "https://mblogthumb-phinf.pstatic.net/MjAyMDA3MjZfMTk2/MDAxNTk1NzM4MTE0MTg0.7G2PZFTiVbr9r-0VRX5OTLAVZroxPRC_A1u1FdL-S9Ug.RRUT4rNhLDCprwkn4PZRwDLY2LrhSAZ2JXdgy0ZQ9KMg.GIF.wogus2003/1595738107665.gif?type=w800",
            "https://i.imgur.com/lYAuac9.gif",
            "https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F2304A04654190CFB16",
            "https://blog.kakaocdn.net/dn/cw6Klq/btqNqBFUmC9/WsumPEAcns5SDSgEL503W1/img.gif",
            "https://i1.ruliweb.com/img/5/7/1/C/571CC988433B9F0024"
        ]
        selected_image = random.choice(anime_images)
        st.markdown(f"""
            <div style="position: fixed; top: 100px; right: 10px;">
                <img src="{selected_image}" width="350" alt="ì• ë‹ˆë©”ì´ì…˜ ì´ë¯¸ì§€">
            </div>
        """, unsafe_allow_html=True)
